# Mode ในการรัน LLM
# DEBUG     รัน LLM ในโหมด Debug หรือ ทดสอบการใช้งาน
# ถ้ากำหนด Option นี้สิ่งที่ตัวระบบจะทำได้แก่
#   1.) จะพิมพ์ Prompt ลงใน text file ชื่อ prompt.log เป็นการเขียนทับ เก็บอยู่ที่ root path ของโครงการนี้
#
#
# RELEASE   รัน LLM ในโหมด Production
# ถ้ากำหนด Option นี้สิ่งที่ตัวระบบจะทำได้แก่
#
#
#
LLM_RUNNING_MODE=DEBUG


# พาร์ทที่เก็บ LLM Model File เอาไว้
# *** ไม่ต้องระบุ \ ต่อท้าย
LLM_MODEL_PATH=models
#LLM_MODEL_PATH=D:\Environment_Owner\Developments\Chatbot\langchain\llm\gpt4all

# ชื่อ LLM Model (Deep Trained) ที่ต้องการโหลดมาใช้ในการถามตอบ
# ให้ระบุเฉพาะชื่อ เช่น ggml-gpt4all-j-v1.3-groovy.bin
# สามารถดาวน์โหลดได้จาก https://gpt4all.io/index.html ไปยังหัวข้อ Model Explorer 
# และเลือกดาวน์โหลด Model ที่ต้องการใช้มา
# *** ใน Version ของ AI นี้จะดาวน์โหลด Model ให้เองในระหว่างการรัน หากยังไม่มีใน "LLM_MODEL_PATH" folder ***
LLM_MODEL_NAME=ggml-gpt4all-j-v1.3-groovy.bin
#LLM_MODEL_NAME=ggml-mpt-7b-instruct.bin
#LLM_MODEL_NAME=ggml-mpt-7b-chat.bin


# ค่าอยู่ระหว่าง 0 - 1 เช่น 0.1, 0.2, 0.3, 0.4, ..., 1
# เป็นค่าความคิดสร้างสรรค์ ในการตอบคำถามของ LLM
# นัยสำคัญของ Fine-tune:
#   ค่า temp จะมีนัยสำคัญในการนำข้อความ ที่ผ่านให้กับ LLM ด้วย Prompt โดย LLM จะนำ Prompt นั้นมาวิเคราะห์ข้อความที่อยู่ภายใน Prompt เพื่อให้ได้ซึ่งคำตอบ
# ตัวอย่างเช่น:
#   Feature: Deposit cash balance Conditions: Users can't deposit cash amounts over 20,000 baht per transaction.\n 
# Users can’t deposit cash amounts over 500,000 baht per day.\n
# Users can deposit cash amounts between 9.00-17.00 If the user’s age is between 12- and 15 years old the fee amount charged is 0.00 baht.\n
#
#  Question: Can I deposit more than 500,000 baht per day?
#  ถ้ากำหนด temp = 0.3 คำตอบที่ได้จะเป็น Yes เสมอ
#  ถ้ากำหนด temp = 0.5 คำตอบที่ได้จะเป็น No แต่รูปแบบประโยค ยังไม่สมบูรณ์
#  ถ้ากำหนด temp = 0.6 คำตอบที่ได้จะเป็น No แต่รูปแบบประโยค ยังไม่สมบูรณ์ แต่ทิศทางดีกว่า temp = 0.5
#  ถ้ากำหนด temp = 0.7 คำตอบที่ได้จะเป็น No แต่รูปแบบประโยค เริ่มสมบูรณ์ขึ้น แต่ยังมีบางส่วนที่ต้องแก้ไข
#  ถ้ากำหนด temp = 0.8 คำตอบที่ได้จะเป็น No รูปแบบประโยคเริ่มถูกต้อง และมีทิศทางทีดี (แนะนำให้ใช้ค่าประมาณนี้)
#
# คำแนะนำและบริษทในการปรับค่า:
#   A. หากนำ LLM นี้ไปใช้งานในเอกสารที่เป็น ลักษณะ Q & A ซึ่งมีคำถามและคำตอบ ในเนื้อหาอยู่แล้ว แนะนำให้ใช้ค่าที่ 0.3 ก็เพียงพอแล้ว
#   B. หากนำ LLM นี้ไปใช้งานในเอกสารที่มีเนื้อหา ที่ต้องการให้ LLM นำเนื้อหานั้นมาวิเคราะห์ข้อความ เพื่อสังเคราะห์ออกมาเป็นคำตอบให้ปรับค่าเป็น 0.8 จะถือเป็นค่าที่เหมาะสม
#
LLM_MODEL_TEMPORATURE=0.4


# พาร์ทที่เก็บไฟล์ของ AI Database ซึ่งในโครงการนี้จะใช้ ChromaDB
# ในการเก็บ และ อ่านเอกสาร และ ถามคำถามเพื่อให้จัดลำดับข้อความที่ใกล้เคียงกับคำตอบมากที่สุด
AI_DB_PERSIST_DIR=ai_db\

# จำนวนรายการ ที่ได้จากการค้นหาของ AI DB
# ซึ่งการค้นหาของ AI DB จะเป็นการจัดลำดับข้อความ ที่มีความสัมพันธ์กับคำถามมากที่สุดเรียงมาเป็น
# ลำดับที่ 1, 2, 3, 4, ...
# ซึ่งผลลัพธ์ทั้งหมดที่ได้จะถูกผ่านส่งให้กับ LLM เพื่อใช้เป็นข้อมูลในการสร้างคำตอบให้กับผู้ถาม
# ค่าที่แนะนำอยู่ที่: 1 หรือ 2
AI_DB_SEARCH_RESULT_RECORD=3

# 02/07/2023:
#   ยกเลิกการ คุณสมบัตินี้ในโครงการนี้ ได้มีการเปลี่ยนวิธีการเก็บข้อมูลของ ChromaDb จากการเรียกผ่าน Langchain มาเป็นเรียกโดยตรง
# ซึ่งมีคุณสมบัติที่ให้ความถูกต้อง และ แม่นยำกว่า ของผลลัพธ์ในการค้นหาคำ โดยใช้ .query(where_documents={"$contains": query_str})
#
# ค่าอยู่ระหว่าง 0 - 2 ซึ่งจะเป็นจุดทศนิยม เช่น 0.7195320129394531 เป็นต้น  
# หรือ กำหนด -1 = ไม่สนใจค่า score
# ความสำคัญ: 
#   ค่ายิ่งน้อย ยิ่งทำให้ความสัมพันธ์ระหว่าง สิ่งที่ถาม กับ เอกสารใน AI DB มีความเกี่ยวข้องกันและทำให้ได้ข้อความที่มีคุณภาพส่งให้กับ LLM
# ความหายของค่า:
#   เป็นค่าระยะห่างระหว่าง คำถาม กับ เนื้อหาใน AI DB
# 
# ค่าที่แนะนำ -1  เพราะจะสนใจรายการค้นหาจาก AI_DB_SEARCH_RESULT_RECORD เพื่อส่งให้กับ LLM สร้างคำตอบ
AI_DB_SEARCH_RESULT_DISTANCING_SCORE_QA_LESS_THAN=-1


# ขนาดของความจุเนื้อหาของเอกสารในแต่ละก้อน หรือ ในแต่ละชิ้น ที่อยู่ภายในแต่ล่ะ Chunk
# ตัวเลขจะเป็น จำนวนคำ ที่เก็บในแต่ล่ะ Chunk 
# ข้อความทราบ:
#   ค่าที่กำหนดในคุณสมบัตินี้ถ้ายิ่งน้อย จำนวน Chunk ก็จะแปรผันตาม เช่น มีเนื้อหาของเอกสารทั้งหมด 10,000 คำ กำหนดขนาดของ Chunk ไว้ที่ 100 = 10,000 / 100 => 100 Chunks เป็นต้น
#
#
AI_DB_STORE_DOCUMENT_SIZE_PER_CHUNK=200

# ชื่อ Model ของ Embeding ชึ่งใช้ SencetenceTranform
# ค้นหารายชื่อ Model Name ได้จากเว็บไซด์ต่อไปนี้
# https://www.sbert.net/docs/pretrained_models.html
EMBEDDING_MODEL_NAME=all-mpnet-base-v2


# พาร์ทที่เก็บรายชื่อเอกสารทั้งหมด ที่โปรแกรมจะอ่านเนื้อหาในไฟล์
# และเก็บลงไปใน AI DB
#SOURCE_DOCUMENT_PATH=source_documents\
SOURCE_DOCUMENT_PATH=source_documents-single\