# Mode ในการรัน LLM
# DEBUG     รัน LLM ในโหมด Debug หรือ ทดสอบการใช้งาน
# ถ้ากำหนด Option นี้สิ่งที่ตัวระบบจะทำได้แก่
#   1.) จะพิมพ์ Prompt ลงใน text file ชื่อ prompt.log เป็นการเขียนทับ เก็บอยู่ที่ root path ของโครงการนี้
#
#
# RELEASE   รัน LLM ในโหมด Production
# ถ้ากำหนด Option นี้สิ่งที่ตัวระบบจะทำได้แก่
#
#
#
LLM_RUNNING_MODE=DEBUG


# พาร์ทที่เก็บ LLM Model File เอาไว้
# *** ไม่ต้องระบุ \ ต่อท้าย
LLM_MODEL_PATH=models
#LLM_MODEL_PATH=D:\Environment_Owner\Developments\Chatbot\langchain\llm\gpt4all

# ชื่อ LLM Model (Deep Trained) ที่ต้องการโหลดมาใช้ในการถามตอบ
# ให้ระบุเฉพาะชื่อ เช่น ggml-gpt4all-j-v1.3-groovy.bin
# สามารถดาวน์โหลดได้จาก https://gpt4all.io/index.html ไปยังหัวข้อ Model Explorer 
# และเลือกดาวน์โหลด Model ที่ต้องการใช้มา
# *** ใน Version ของ AI นี้จะดาวน์โหลด Model ให้เองในระหว่างการรัน หากยังไม่มีใน "LLM_MODEL_PATH" folder ***
LLM_MODEL_NAME=ggml-gpt4all-j-v1.3-groovy.bin
#LLM_MODEL_NAME=ggml-mpt-7b-instruct.bin
#LLM_MODEL_NAME=ggml-mpt-7b-chat.bin


# ค่าอยู่ระหว่าง 0 - 1 เช่น 0.1, 0.2, 0.3, 0.4, ..., 1
# เป็นค่าความคิดสร้างสรรค์ ในการตอบคำถามของ LLM
# ข้อความระวัง:
#  ค่ายิ่งใกล้กับ 1 มากเท่าไหร่คำถามเดียวกัน คำตอบจะได้ไม่เหมือนกัน ในการถามแต่ละครั้ง แต่เนื้อหาของคำตอบจะไปในทางเดียวกัน
#  ค่ายิ่งใกล้กับ 1 ทำให้ได้คำตอบที่สร้างสรรค์มากขึ้นเท่านั้น
LLM_MODEL_TEMPORATURE=0.3


# พาร์ทที่เก็บไฟล์ของ AI Database ซึ่งในโครงการนี้จะใช้ ChromaDB
# ในการเก็บ และ อ่านเอกสาร และ ถามคำถามเพื่อให้จัดลำดับข้อความที่ใกล้เคียงกับคำตอบมากที่สุด
AI_DB_PERSIST_DIR=ai_db\

# จำนวนรายการ ที่ได้จากการค้นหาของ AI DB
# ซึ่งการค้นหาของ AI DB จะเป็นการจัดลำดับข้อความ ที่มีความสัมพันธ์กับคำถามมากที่สุดเรียงมาเป็น
# ลำดับที่ 1, 2, 3, 4, ...
# ซึ่งผลลัพธ์ทั้งหมดที่ได้จะถูกผ่านส่งให้กับ LLM เพื่อใช้เป็นข้อมูลในการสร้างคำตอบให้กับผู้ถาม
# ค่าที่แนะนำอยู่ที่: 1 หรือ 2
AI_DB_SEARCH_RESULT_RECORD=1

# 02/07/2023:
#   ยกเลิกการ คุณสมบัตินี้ในโครงการนี้ ได้มีการเปลี่ยนวิธีการเก็บข้อมูลของ ChromaDb จากการเรียกผ่าน Langchain มาเป็นเรียกโดยตรง
# ซึ่งมีคุณสมบัติที่ให้ความถูกต้อง และ แม่นยำกว่า ของผลลัพธ์ในการค้นหาคำ โดยใช้ .query(where_documents={"$contains": query_str})
#
# ค่าอยู่ระหว่าง 0 - 2 ซึ่งจะเป็นจุดทศนิยม เช่น 0.7195320129394531 เป็นต้น  
# หรือ กำหนด -1 = ไม่สนใจค่า score
# ความสำคัญ: 
#   ค่ายิ่งน้อย ยิ่งทำให้ความสัมพันธ์ระหว่าง สิ่งที่ถาม กับ เอกสารใน AI DB มีความเกี่ยวข้องกันและทำให้ได้ข้อความที่มีคุณภาพส่งให้กับ LLM
# ความหายของค่า:
#   เป็นค่าระยะห่างระหว่าง คำถาม กับ เนื้อหาใน AI DB
# 
# ค่าที่แนะนำ -1  เพราะจะสนใจรายการค้นหาจาก AI_DB_SEARCH_RESULT_RECORD เพื่อส่งให้กับ LLM สร้างคำตอบ
AI_DB_SEARCH_RESULT_DISTANCING_SCORE_QA_LESS_THAN=-1


# ขนาดของความจุเนื้อหาของเอกสารในแต่ละก้อน หรือ ในแต่ละชิ้น ที่อยู่ภายในแต่ล่ะ Chunk
# ตัวเลขจะเป็น จำนวนคำ ที่เก็บในแต่ล่ะ Chunk 
# ข้อความทราบ:
#   ค่าที่กำหนดในคุณสมบัตินี้ถ้ายิ่งน้อย จำนวน Chunk ก็จะแปรผันตาม เช่น มีเนื้อหาของเอกสารทั้งหมด 10,000 คำ กำหนดขนาดของ Chunk ไว้ที่ 100 = 10,000 / 100 => 100 Chunks เป็นต้น
#
#
AI_DB_STORE_DOCUMENT_SIZE_PER_CHUNK=500

# ชื่อ Model ของ Embeding ชึ่งใช้ SencetenceTranform
# ค้นหารายชื่อ Model Name ได้จากเว็บไซด์ต่อไปนี้
# https://www.sbert.net/docs/pretrained_models.html
EMBEDDING_MODEL_NAME=all-mpnet-base-v2


# พาร์ทที่เก็บรายชื่อเอกสารทั้งหมด ที่โปรแกรมจะอ่านเนื้อหาในไฟล์
# และเก็บลงไปใน AI DB
SOURCE_DOCUMENT_PATH=source_documents\
#SOURCE_DOCUMENT_PATH=source_documents-single\